{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLWAmEvqmFze",
        "outputId": "f36afdbc-3d0b-4789-edb8-4cef5ce87d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Dog_heart_project/Dog_heart\"\n",
        "test_path = \"/content/drive/MyDrive/Dog_heart_project/Test\"\n"
      ],
      "metadata": {
        "id": "FVW8_prSmQwW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# List subfolders in Train and Valid\n",
        "print(\"Train Classes:\", os.listdir(f\"{dataset_path}/Train\"))\n",
        "print(\"Valid Classes:\", os.listdir(f\"{dataset_path}/Valid\"))\n",
        "\n",
        "# List sample images from each class\n",
        "print(\"Sample images from Train/Normal:\", os.listdir(f\"{dataset_path}/Train/Normal\")[:5])\n",
        "print(\"Sample images from Train/Small:\", os.listdir(f\"{dataset_path}/Train/Small\")[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-ous-C9mT7M",
        "outputId": "bddc86b2-20c6-4f5f-fdbf-4dfb3b53dbb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Classes: ['Small', 'Normal', 'Large']\n",
            "Valid Classes: ['Large', 'Small', 'Normal']\n",
            "Sample images from Train/Normal: ['303.png', '305.png', '306.png', '313.png', '316.png']\n",
            "Sample images from Train/Small: ['307.png', '325.png', '326.png', '108.png', '119.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "# Force images to RGB\n",
        "def pil_loader_rgb(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "# Define transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Load datasets\n",
        "train_dataset = ImageFolder(root=f\"{dataset_path}/Train\", transform=transform, loader=pil_loader_rgb)\n",
        "valid_dataset = ImageFolder(root=f\"{dataset_path}/Valid\", transform=transform, loader=pil_loader_rgb)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(\"Classes:\", train_dataset.class_to_idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF2FYLlBo5sJ",
        "outputId": "adcb6192-2f3c-4266-c9a9-5457b948ac77"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: {'Large': 0, 'Normal': 1, 'Small': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedCNN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 16 * 16, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n"
      ],
      "metadata": {
        "id": "ajPYYtiRo9e2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ImprovedCNN().to(device)  # <-- Move model after device is defined\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "id": "XSxWdSklpAtK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, valid_loader, epochs=60):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, valid_loader, epochs=60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrKCN-Zspcaf",
        "outputId": "53e66530-599a-46c0-d812-4edbf8921d04"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60, Loss: 1.0860\n",
            "Epoch 2/60, Loss: 1.0190\n",
            "Epoch 3/60, Loss: 0.9779\n",
            "Epoch 4/60, Loss: 0.9723\n",
            "Epoch 5/60, Loss: 0.9591\n",
            "Epoch 6/60, Loss: 0.9489\n",
            "Epoch 7/60, Loss: 0.9373\n",
            "Epoch 8/60, Loss: 0.9271\n",
            "Epoch 9/60, Loss: 0.9083\n",
            "Epoch 10/60, Loss: 0.8967\n",
            "Epoch 11/60, Loss: 0.8632\n",
            "Epoch 12/60, Loss: 0.8610\n",
            "Epoch 13/60, Loss: 0.8386\n",
            "Epoch 14/60, Loss: 0.8259\n",
            "Epoch 15/60, Loss: 0.8128\n",
            "Epoch 16/60, Loss: 0.7898\n",
            "Epoch 17/60, Loss: 0.7952\n",
            "Epoch 18/60, Loss: 0.7751\n",
            "Epoch 19/60, Loss: 0.7639\n",
            "Epoch 20/60, Loss: 0.7638\n",
            "Epoch 21/60, Loss: 0.7345\n",
            "Epoch 22/60, Loss: 0.7470\n",
            "Epoch 23/60, Loss: 0.7203\n",
            "Epoch 24/60, Loss: 0.7387\n",
            "Epoch 25/60, Loss: 0.7158\n",
            "Epoch 26/60, Loss: 0.7032\n",
            "Epoch 27/60, Loss: 0.7240\n",
            "Epoch 28/60, Loss: 0.7136\n",
            "Epoch 29/60, Loss: 0.6882\n",
            "Epoch 30/60, Loss: 0.6834\n",
            "Epoch 31/60, Loss: 0.6873\n",
            "Epoch 32/60, Loss: 0.6759\n",
            "Epoch 33/60, Loss: 0.6857\n",
            "Epoch 34/60, Loss: 0.6977\n",
            "Epoch 35/60, Loss: 0.6681\n",
            "Epoch 36/60, Loss: 0.6681\n",
            "Epoch 37/60, Loss: 0.6620\n",
            "Epoch 38/60, Loss: 0.6678\n",
            "Epoch 39/60, Loss: 0.6582\n",
            "Epoch 40/60, Loss: 0.6813\n",
            "Epoch 41/60, Loss: 0.6525\n",
            "Epoch 42/60, Loss: 0.6506\n",
            "Epoch 43/60, Loss: 0.6517\n",
            "Epoch 44/60, Loss: 0.6395\n",
            "Epoch 45/60, Loss: 0.6346\n",
            "Epoch 46/60, Loss: 0.6565\n",
            "Epoch 47/60, Loss: 0.6343\n",
            "Epoch 48/60, Loss: 0.6389\n",
            "Epoch 49/60, Loss: 0.6184\n",
            "Epoch 50/60, Loss: 0.6367\n",
            "Epoch 51/60, Loss: 0.6220\n",
            "Epoch 52/60, Loss: 0.6218\n",
            "Epoch 53/60, Loss: 0.6196\n",
            "Epoch 54/60, Loss: 0.6279\n",
            "Epoch 55/60, Loss: 0.6263\n",
            "Epoch 56/60, Loss: 0.6215\n",
            "Epoch 57/60, Loss: 0.6212\n",
            "Epoch 58/60, Loss: 0.6239\n",
            "Epoch 59/60, Loss: 0.6101\n",
            "Epoch 60/60, Loss: 0.6111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, valid_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print(f\"Final Model Accuracy: {(100 * correct / total):.2f}%\")\n",
        "\n",
        "evaluate_model(model, valid_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivklfbVHxORI",
        "outputId": "00e8b7a1-c169-417a-dfd4-ad6e44947f2f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Model Accuracy: 71.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n"
      ],
      "metadata": {
        "id": "fg6hyIYXbIn9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use official 400 image names expected by MATLAB checker\n",
        "allowed_filenames = [\n",
        "    \"1621.png\", \"1622.png\", \"81_12_1.png\", \"94.png\", \"100.png\",  # ... etc (400 total)\n",
        "    # Paste the rest of the filenames here from professor's list\n",
        "]\n"
      ],
      "metadata": {
        "id": "MwahUYEZbO8T"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/clean_allowed_filenames.txt\", \"r\") as f:\n",
        "    allowed_filenames = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "print(\"Filtered filenames:\", len(allowed_filenames))  # Should print 400\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfnLCP-odZkS",
        "outputId": "6f08eb3c-e8e1-494f-93e5-ef235e5233ff"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered filenames: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "# Define test transform (no augmentation)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# Filter and sort test filenames\n",
        "test_filenames = sorted([f for f in os.listdir(test_path) if f in allowed_filenames])\n",
        "\n",
        "def predict_selected_images(model, test_path, transform, filenames):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for img_name in filenames:\n",
        "        image = Image.open(os.path.join(test_path, img_name)).convert(\"RGB\")\n",
        "        image = transform(image).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "        predictions.append((img_name, predicted.item()))\n",
        "    return predictions\n",
        "\n",
        "# Generate predictions\n",
        "test_predictions = predict_selected_images(model, test_path, test_transform, test_filenames)\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(test_predictions, columns=[\"filename\", \"label\"])\n",
        "df.to_csv(\"/content/test_predictions_final_400.csv\", index=False)\n",
        "\n",
        "print(\"CSV saved as test_predictions_final_400.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS2waXN-bRzC",
        "outputId": "d560b006-190a-4457-9314-52e719a7ad1c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV saved as test_predictions_final_400.csv\n"
          ]
        }
      ]
    }
  ]
}